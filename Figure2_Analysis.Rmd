---
title: "Figure2_Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "Figure2_Analysis"
author: "Sarah Tennant"
date: "18/11/2020"
output: html_document
---

## Analysis of neurons recorded from the medial entorhinal cortex during virtual navigation

The aim of this analysis is to identify all ramp cells within a specified dataset (all mice or all days for one mouse) and perform analysis to investigate their firing properties. 
1. Identify cells that represent location by ramping their firing rate using LM modeling 
2. Subset data by model fit (r2 value)
3. Compare firing rates of cells in this group


To set up, including loading packages and data, first run SetUp.Rmd.


### ----------------------------------------------------------------------------------------- ###

## Average firing rate (for LM modelling)

The linear model uses firing rate data binned in space. For this we want to load average firing rate over trials from the data frame for each cluster. 
- Map over "Rates_averaged" column and extract averaged rates
- Add position for each point ( data is binned into 200, 1 cm bins )
- Insert result back into dataframe
- Do this for beaconed, nonbeaconed and probe trials
- Do this for shuffled spike rate (beaconed)

1. Write function to add position
```{r}
add_position <- function(df, session_id, cluster_id) {
  df <- tibble(Rates = unlist(df), Position = rep(1:200)) 
  if(all(is.na(df$Rates))){print(paste0("All NAs. Session: ", session_id, ". Cluster:", cluster_id))}
  df
}
```

2. Run on dataframe : Average trials with reward

input columns: 
Rates_averaged_rewarded_b = beaconed trials
Rates_averaged_rewarded_nb = non-beaconed / probe trials
Rates_averaged_rewarded_p = probe trials
```{r}
spatial_firing <- spatial_firing %>%
  mutate(asr_b_rewarded = pmap(list(Rates_averaged_rewarded_b, session_id, cluster_id), add_position),
         asr_nb_rewarded = pmap(list(Rates_averaged_rewarded_nb, session_id, cluster_id), add_position),
         asr_p_rewarded = pmap(list(Rates_averaged_rewarded_p, session_id, cluster_id), add_position)
         )
```


### ----------------------------------------------------------------------------------------- ###

# Run simple linear model to examine relationship between firing rate and position
_note:for now we are only interested in the outbound region of the track (30 - 90 cm)_

1. Make function to run linear model
```{r}
lm_helper <- function(df,
                      startbin,
                      endbin) {
 # Check for NAs
  if (any(is.na(df$Rates))) {
    df <-
      tibble(
        r.squared = c(NA),
        p.value = c(NA),
        intercept = c(NA),
        slope = c(NA)
      )
    return(df)
  }

  df <- df %>%
    subset(Position >= startbin & Position <= endbin)
  df_fit <- lm(Rates ~ Position, data = df, na.action = na.exclude)

  # get the model parameters
  params <- select(glance(df_fit), r.squared, p.value)
  # get the coefficients
  coeffs <- tidy(df_fit)
  # combine the parameters and coefficients
  params$intercept <- coeffs$estimate[[1]]
  params$slope <- coeffs$estimate[[2]]
  return(params)
}
```

2. run lm on all cells.
Removes any previously generated results (select), fits the data (mutate) and then adds model outputs as columns to spatial firing (unnest_wider).
```{r}
spatial_firing <- spatial_firing %>%
  select(-contains('asr_b_o_rewarded_fit_')) %>%
  select(-contains('asr_nb_o_rewarded_fit_')) %>%
  select(-contains('asr_p_o_rewarded_fit_')) %>%
  mutate(asr_b_o_rewarded_fit = pmap(list(asr_b_rewarded, 30, 90), lm_helper),
         asr_nb_o_rewarded_fit = pmap(list(asr_nb_rewarded, 30, 90), lm_helper),
         asr_p_o_rewarded_fit = pmap(list(asr_p_rewarded, 30, 90), lm_helper)) %>%
  unnest_wider(asr_b_o_rewarded_fit, names_sep = "_", names_repair = "universal") %>%
  unnest_wider(asr_nb_o_rewarded_fit, names_sep = "_", names_repair = "universal") %>%
  unnest_wider(asr_p_o_rewarded_fit, names_sep = "_", names_repair = "universal")
```

Linear model results are stored in:
spatial_firing$asr_b_o_rewarded_fit_pval
spatial_firing$asr_b_o_rewarded_fit_slope
spatial_firing$asr_b_o_rewarded_fit_r.squared


3. Plot coefficients of model (slope and r2 value) for each neuron
```{r}
ggplot(data=spatial_firing, aes(x = asr_b_o_rewarded_fit_slope,
                                y = asr_b_o_rewarded_fit_r.squared)) +
    coord_cartesian(xlim = c(-0.7,0.7), ylim = c(0,1)) +
    geom_point() +
    xlab("\nslope") +
    ylab("R2") +
    theme_classic() +
    scale_color_manual(values=c("grey82", "grey32", "violetred2", "chartreuse3")) +
    theme(axis.text.x = element_text(size=12),
          axis.text.y = element_text(size=12),
          legend.position="bottom", 
          legend.title = element_blank(),
          text = element_text(size=12), 
          legend.text=element_text(size=12), 
          axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) 
ggsave(file = "plots/shuff_lm_allcells.png", width = 4, height = 5)
```
,


### ----------------------------------------------------------------------------------------- ###

## Identification of ramp cells in dataset 

Ramp like cells are identified by whether the coefficients of the linear model lie outside the 95% confidence intervals of the same result from 1000 shuffled datasets


1. write function to generate 1000 shuffles
- shuffles spikes using sample() function
- runs lm
- extracts coefficients
- stores coefficients for each 1000 shuffles (less memory than saving 1000 shuffles)
```{r}
#library(gdata)
# shuffles defines the number of shuffes. Use a smaller value for testing.
shuffle_rates <- function(df, startbin, endbin, shuffles = 1000) {
  df_modified <- data.frame(neuron=as.numeric(),
                 slope=as.numeric(), 
                 rsquared=as.numeric(), 
                 pval=vector())
  names(df_modified) <- c("neuron", "slope", "rsquared", "pval")
  x <- 1
  repeat {
  shuff_df <- tibble(Rates = sample(as.vector(unlist(df)),replace = TRUE, prob = NULL), Position = c(1:200))
  df_mod <- lm_helper(shuff_df, startbin, endbin)
  data <- data.frame(as.numeric(x), df_mod$slope, df_mod$r.squared, df_mod$p.value)
  names(data) <- c("neuron", "slope", "r.squared", "p.value")
  df_modified <- rbind(df_modified,data)

  x = x+1
  if (x == shuffles){ 
  break
  }
  }
return(df_modified)
}
```

2. Run on example neuron (beaconed and non-beaconed trials)
```{r}
# Uncomment the line below to load pre-saved shuffled data
# spatial_firing <- readRDS(SpatialFiring_with_1000_shuffles.Rda)

# Check to see if the column shuffle_results exists. If it does then don't run again.
shuffles <- 10
if(!"shuffle_results_b_o" %in% colnames(spatial_firing)) {
  spatial_firing <- spatial_firing %>%
    mutate(shuffle_results_b_o = pmap(list(Rates_averaged_rewarded_b, 30, 90, shuffles), shuffle_rates)) %>%
    mutate(shuffle_results_nb_o = pmap(list(Rates_averaged_rewarded_nb, 30, 90, shuffles), shuffle_rates)) %>%
    mutate(shuffle_results_p_o = pmap(list(Rates_averaged_rewarded_p, 30, 90, shuffles), shuffle_rates))
}
# Uncomment the line below to save spatial firing at this point.
saveRDS(spatial_firing, "SpatialFiring_with_100_shuffles.Rda")
# To reload use
#spatial_firing <- readRDS("SpatialFiring_with_100_shuffles.Rda")
# And use this to save a truncated version. Useful for testing code.
# saveRDS(slice_head(spatial_firing, n = 5), "SpatialFiring_with_1000_shuffles_trunc.Rda")
```


### ---------------------------------------------------------------------------- ### 

### classify neurons based on shuffle activity

If outside the 95% of the shuffled dataset, a neuron is considered to have ramp like activity along the track. 

First, extract the 5 % and 95 % limits of 1000 shuffles for each neuron 

1. write function to find min and max slope for shuffled datasets 
```{r}

extract_min_shuffle_slopes <- function(df){
  df <- tibble(slopes = unlist(df$slope), r.squared = unlist(df$r.squared))
  if (all(is.na(df$slopes))) {
    return(NA)
  }
  min_slope_o <- quantile(as.numeric(unlist(df$slopes)), c(.05, .95)) [[1]][1]
  return(min_slope_o)
}

extract_max_shuffle_slopes <- function(df){
  df <- tibble(slopes = unlist(df$slope), r.squared = unlist(df$r.squared))
    if (all(is.na(df$slopes))) {
    return(NA)
  }
  max_slope_o <- quantile(as.numeric(unlist(df$slopes)), c(.05, .95)) [[2]][1]
  return(max_slope_o)
}

```

2. Run on all neurons
```{r}
spatial_firing <- spatial_firing %>%
  mutate(shuffle_min_slope_b_o = map_dbl(shuffle_results_b_o, extract_min_shuffle_slopes),
         shuffle_max_slope_b_o = map_dbl(shuffle_results_b_o, extract_max_shuffle_slopes),
         shuffle_min_slope_nb_o = map_dbl(shuffle_results_nb_o, extract_min_shuffle_slopes),
         shuffle_max_slope_nb_o = map_dbl(shuffle_results_nb_o, extract_max_shuffle_slopes),
         shuffle_min_slope_p_o = map_dbl(shuffle_results_p_o, extract_min_shuffle_slopes),
         shuffle_max_slope_p_o = map_dbl(shuffle_results_p_o, extract_max_shuffle_slopes))

```


We also want to extract slopes, r2 and pvalues of the 1000 shuffles for each neuron

1. Extract shuffle results (slopes and r2 for each shuffle)

2. run on all neurons
```{r}
spatial_firing <- spatial_firing %>%
  select(-contains('shuffle_results_b_o_')) %>%
  select(-contains('shuffle_results_nb_o_')) %>%
  select(-contains('shuffle_results_p_o_')) %>%
  unnest_wider(shuffle_results_b_o, names_sep = "_", names_repair = "universal") %>%
  unnest_wider(shuffle_results_nb_o, names_sep = "_", names_repair = "universal") %>%
  unnest_wider(shuffle_results_p_o, names_sep = "_", names_repair = "universal")
```


Then we want to correct the pvals of the lm, to account for multiple comparisons

1. put all pvalues into tibble then adjust using p.adjust from - package
```{r}
p_vals_b <- tibble(pvals = spatial_firing$asr_b_o_rewarded_fit_p.value)
adu_p_b <- tibble(adjust_pval_b_o = p.adjust(p_vals_b$pvals, "BH"))

```

2. bind new adjusted pvalues to dataframe
```{r}
spatial_firing <- cbind(spatial_firing, adu_p_b)
```

3. do the same for non-beaconed trials
```{r}
p_vals_nb <- tibble(pvals = spatial_firing$asr_nb_o_rewarded_fit_p.value)
adu_p_nb <- tibble(adjust_pval_nb_o = p.adjust(p_vals_nb$pvals, "BH"))
spatial_firing <- cbind(spatial_firing, adu_p_nb)
```

3. do the same for probe trials 
```{r}
p_vals_p <- tibble(pvals = spatial_firing$asr_p_o_rewarded_fit_p.value)
adu_p_p <- tibble(adjust_pval_p_o = p.adjust(p_vals_p$pvals, "BH"))
spatial_firing <- cbind(spatial_firing, adu_p_p)
```


### ----------------------------------------------------------------------------------------- ### 

Now we want to classify neurons, taking the adjusted significance into account

```{r}
compare_slopes <-
  function(min_slope = 1,
           max_slope = 1,
           slope = 1,
           pval = 1) {
    if (any(is.na(list(min_slope, max_slope, slope, pval)))) {
      return("NAs")
    }
    if (pval > 0.01) {
      return("Unclassified")
    } else if (slope < min_slope & pval < 0.01) {
      return("Negative")
    } else if (slope > max_slope & pval < 0.01) {
      return("Positive")
    } else if (slope > min_slope & slope < max_slope) {
      return("Unclassified")
    } else {
      return("Unclassified")
    }
  }

spatial_firing <- spatial_firing %>%
  mutate(
    lm_group_b = pmap(
      list(
        shuffle_min_slope_b_o,
        shuffle_max_slope_b_o,
        asr_b_o_rewarded_fit_slope,
        adjust_pval_b_o
      ),
      compare_slopes
    ),
    lm_group_nb = pmap(
      list(
        shuffle_min_slope_nb_o,
        shuffle_max_slope_nb_o,
        asr_nb_o_rewarded_fit_slope,
        adjust_pval_nb_o
      ),
      compare_slopes
    ),
    lm_group_p = pmap(
      list(
        shuffle_min_slope_p_o,
        shuffle_max_slope_p_o,
        asr_p_o_rewarded_fit_slope,
        adjust_pval_p_o
      ),
      compare_slopes
    )
  )

```


Linear model classification is stored in:
spatial_firing$lm_group_b
spatial_firing$lm_group_nb
spatial_firing$lm_group_p


### -------------------------------------------------------------------------------------------------------------------- ### 


Now we want to visualise the coefficients of all neurons and all shuffled datasets

1. extract shuffled values into tibble _nb needed because each 1000 shuffled datasets are nested for each neuron_
```{r}
shuff_slopes <- tibble(slopes = unlist(spatial_firing$shuffle_results_b_o_slope), r2 = unlist(spatial_firing$shuffle_results_b_o_r.squared))
```

2. plot real and shuffled coefficients 
```{r}
ggplot(data=shuff_slopes, aes(x = slopes, y = r2), color="grey32", fill="grey32") + 
    coord_cartesian(xlim = c(-0.6,0.6), ylim = c(0,1)) +
    geom_point(alpha=.4) + 
    geom_point(data=spatial_firing, aes(x = asr_b_o_rewarded_fit_slope, y = asr_b_o_rewarded_fit_r.squared, color=factor(unlist(lm_group_b)))) +
    #geom_point(data=shuff_slopes, aes(x = slopes, y = r2), color="grey32") +
    xlab("\nslope") +
    ylab("R2") +
    scale_color_manual(values=c("violetred2", "chartreuse3", "grey82")) +
    theme_classic() +
    theme(axis.text.x = element_text(size=12),
          axis.text.y = element_text(size=12),
          legend.position="bottom", 
          legend.title = element_blank(),
          text = element_text(size=12), 
          legend.text=element_text(size=12), 
          axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) 
ggsave(file = "plots/LMOut_coefficients.png", width = 4, height = 5)

```



Now lets find and plot the proportion of cells that pass criteria according to our classification

1. extract proportion of cells that meet each criteria
```{r}
# positive homebound slopes
start <- nrow(subset(spatial_firing, lm_group_b == "Negative"))/nrow(spatial_firing)*100
reward <- nrow(subset(spatial_firing, lm_group_b == "Positive"))/nrow(spatial_firing)*100
nonslope <- nrow(subset(spatial_firing, lm_group_b == "Unclassified"))/nrow(spatial_firing)*100


```

2. Put into a tibble 
```{r}
proportions_mixed_ramps <- tibble(perc=c(start, reward, nonslope), ramp_id= c("Start", "ToReward", "Unclassified"),ramp_type = c("Start", "ToReward", "Unclassified"))
```

3. Plot bar graph of proportions
```{r}
ggplot(proportions_mixed_ramps, aes(x= ramp_type, y = perc, fill=factor(ramp_id))) +
  geom_bar(stat="identity",width = 0.9, alpha = .4) +
  labs(y = "Percent") +
  scale_fill_manual(values=c("violetred2", "chartreuse3", "grey62")) +
  theme_classic() +
  theme(axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16),
        legend.position="bottom", 
        legend.title = element_blank(),
        text = element_text(size=16), 
        legend.text=element_text(size=16), 
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +

ggsave(file = "plots/LMOut_proportions_rewarded.png", width = 3, height = 6)

```


We might also want to visualise the coefficients for the real and shuffled dataset as a histogram.

1. First, make stacked histogram of slope values for real dataset
```{r}
level_order <- c("Negative", "Positive", "Unclassified")

ggplot(data=spatial_firing, aes(x= asr_b_o_rewarded_fit_slope, fill=factor(unlist(lm_group_b), level = level_order))) + #fill=factor(unlist(lm_group_b))
  coord_cartesian(xlim = c(-0.6,0.6)) +
  geom_histogram(aes(y=..count../sum(..count..)), binwidth=0.01) +
  ylab("Proportion") +
  scale_fill_manual(values=c("violetred2", "chartreuse3", "grey62")) +
  theme_classic() +
  theme(axis.text.x = element_text(size=14),
        axis.text.y = element_text(size=14),
        legend.title = element_blank(),
        legend.position = "none",
        text = element_text(size=14),
        legend.text=element_text(size=14),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
ggsave(file = "plots/LMOut_histogram_slopes.png", width = 4.5, height = 2)

```

2. same as above but for shuffled datasets
```{r}
ggplot(data=shuff_slopes, aes(x = slopes)) +
    coord_cartesian(xlim = c(-0.6,0.6)) +
    geom_histogram(aes(y=..count../sum(..count..)), binwidth=0.01) +
    labs(x = "Slope (Hz/cm)") +
    ylab("Proportion") +
    theme_classic() +
    theme(axis.text.x = element_text(size=14),
          axis.text.y = element_text(size=14),
          legend.position="bottom",
          legend.title = element_blank(),
          text = element_text(size=14),
          legend.text=element_text(size=14),
          axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)))
ggsave(file = "plots/LMOut_shuffle_datasets_histogram.png", width = 4.5, height = 2)
```


3. Make stacked histogram of rsquared values for real dataset

```{r}
level_order <- c("Negative", "Positive", "Unclassified")

ggplot(data=spatial_firing, aes(x= asr_b_o_rewarded_fit_r.squared, fill=factor(unlist(lm_group_b), level = level_order))) + #fill=factor(unlist(lm_group_b))
  coord_cartesian(xlim = c(0,1)) +
  geom_histogram(aes(y=..count../sum(..count..)), binwidth=0.01) +
  ylab("Proportion") +
  xlab("\nrsquared") +
  scale_fill_manual(values=c("violetred2", "chartreuse3", "grey62")) +
  theme_classic() +
  theme(axis.text.x = element_text(size=14),
        axis.text.y = element_text(size=14),
        legend.title = element_blank(),
        legend.position = "none",
        text = element_text(size=14),
        legend.text=element_text(size=14),
        axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
ggsave(file = "plots/LMOut_histogram_r2.png", width = 4.5, height = 2)


```

4. Same as above but for shuffled datasets

```{r}
ggplot(data=shuff_slopes, aes(x = r2)) +
    coord_cartesian(xlim = c(0,1)) +
    geom_histogram(aes(y=..count../sum(..count..)), binwidth=0.01) +
    xlab(expression(R^2)) +
    ylab("Proportion") +
    theme_classic() +
    theme(axis.text.x = element_text(size=14),
          axis.text.y = element_text(size=14),
          legend.position="none",
          legend.title = element_blank(),
          text = element_text(size=14),
          legend.text=element_text(size=14),
          axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)))
ggsave(file = "plots/LMOut_shuffle_r2_datasets_histogram.png", width = 4.5, height = 2)
```


Plot distribution of the shuffled data

1. Extract shuffled slopes and rsquared values. This is already done above.

2. Plot in scatter
```{r}
ggplot(data=shuff_slopes, aes(x = slopes, y = r2)) +
    geom_point() +
    xlab("\nslope") +
    ylab("R2") +
    theme_classic() +
    theme(axis.text.x = element_text(size=12),
          axis.text.y = element_text(size=12),
          legend.position="bottom",
          legend.title = element_blank(),
          text = element_text(size=12),
          legend.text=element_text(size=12),
          axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)))
ggsave(file = "plots/LMOut_shuffle_datasets.png", width = 3, height = 3)

```

### ----------------------------------------------------------------------------------------- ###


# Plot heat map of firing rate across location for all neurons

First, reorder the dataframe with ramps according to slope.
_For start ramps, steepest slope should be negative - thus will have the highest cluster id_

```{r}
start_ramps<-spatial_firing[order(spatial_firing$asr_b_o_rewarded_fit_slope),]
start_ramp_number = nrow(start_ramps)
new_cluster_id = seq(from = 1, to = start_ramp_number, by = 1)
start_ramps <- cbind(start_ramps, new_cluster_id)
```

Then, scale firing rate for all neurons

1. make function to load rates and normalise
2. Run on dataframe 
```{r}
normalise_rates_outbound <- function(df){
  df <- tibble(Rates = unlist(df), Position = rep(1:200))
  df <- df %>%
    filter(Position >=30, Position <= 90)
  x <- normalit(df$Rates)
  return(x)
}

start_ramps <- start_ramps %>%
  mutate(normalised_rates_o = map(Rates_averaged_rewarded_b, normalise_rates_outbound))

```

Add position to normalised rates for plotting
```{r}
add_position <- function(df) {
  df <- tibble(Rates = unlist(df), Position = rep(30:90))
}

start_ramps <- start_ramps %>%
  mutate(normalised_rates_o = map(normalised_rates_o, add_position))

```

Extract columns (normalised rates) for plotting into a tibble
```{r}
concat_firing_start <- unnest(select(start_ramps, new_cluster_id, normalised_rates_o))
```

Now the firing rates have been normalised and the data in the right format we want to make annotations for the heatmap

First, extract ramp score from the dataframe for annotating heatmap
_since its a list of three (outbound/homebound/all) we extract the first one_
```{r}
start_ramps <- start_ramps %>%
  mutate(start_ramp_score = map(ramp_score, ~.x[1]))

```

Put ramp scores alongisde lm results and brain region classifier in tibble for annotating heatmap
```{r}
ramp_result <- tibble(ramp_score = as.numeric(start_ramps$start_ramp_score))
brain_region <- tibble(region = as.character(start_ramps$brain_region))
lm_result <- tibble(result = as.character(start_ramps$lm_group_b))
cluster_result <- tibble(result = as.character(start_ramps$new_cluster_id))
```

Now we can plot the heatmap with annotations using pheatmap
```{r}
#convert data to wide format
wide_DF <- concat_firing_start %>% spread(Position, Rates)

# Generte data (modified the mydf slightly)
colnames(wide_DF) <- c("new_cluster_id", rep(30:90, times=1))
rownames(wide_DF) <- paste("neuron", 1:max(start_ramps$new_cluster_id), sep="_")

#remove unused column
name <- "new_cluster_id"
wide_DF <- wide_DF %>% select(-one_of(name))

# data for annotation rows in seperate dataframe
mydf <- data.frame(row.names = paste("neuron", 1:max(start_ramps$new_cluster_id), sep="_"), catagory = lm_result, region = brain_region, ramp_score=ramp_result)

# change the color of annotation to what you want: (eg: "navy", "darkgreen")
Var1        <- c("violetred2", "black", "chartreuse3")
names(Var1) <- c("Negative", "Unclassified", "Positive")

Var2        <- c("springgreen4", "deepskyblue1", "firebrick", "violetred2" )
names(Var2) <- c("PS", "UN", "V1", "PreS")

anno_col <- list(result = Var1, region = Var2, ramp_score = brewer.pal(11,"RdBu"))

#annotation_row = mydf, annotation_colors = anno_col, show_rownames = F, show_colnames = F
myheatmap<-pheatmap(wide_DF,cluster_cols = F, cluster_rows = F, annotation_row = mydf, show_rownames = F, show_colnames = F )

myheatmap<-pheatmap(wide_DF,cluster_cols = F, cluster_rows = F, show_rownames = F, show_colnames = F )
```

Save the heatmap (bit of a nightmare here...)
```{r}
save_pheatmap_png <- function(x, filename, width=1300, height=2500, res = 250) {
  png(filename, width = width, height = height, res = res)
  grid::grid.newpage()
  grid::grid.draw(x$gtable)
  dev.off()
}
 
save_pheatmap_png(myheatmap, "my_heatmap_all.png")
```


```{r}
saveRDS(spatial_firing, file="All_Results.Rda")

```

